{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WillLacey/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for utils.py\n",
    "import multiprocessing.pool\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# for vgg.py\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for decoder.py\n",
    "from keras.layers import Input, Conv2D, UpSampling2D\n",
    "\n",
    "# for model.py \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, Input\n",
    "import keras.backend as K\n",
    "\n",
    "# for train.py\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# for evaluate-decoder.py\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# for style.py\n",
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vgg\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "\n",
    "# for model.py\n",
    "LAMBDA=1\n",
    "\n",
    "# for train.py \n",
    "TRAIN_PATH = 'data/input/train'\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 4\n",
    "epochs = 2\n",
    "target_layer = 1 # also for evaluate-decoder\n",
    "image_path = \"data/input/content.png\"\n",
    "preview_dir_path = \"data/output\"\n",
    "\n",
    "# for evaluate-decoder.py\n",
    "DECODER_PATH = 'models/first_decoder_1.h5'\n",
    "INPUT_IMG_PATH = content_image_path\n",
    "OUTPUT_IMG_PATH = 'data/output/out.png'\n",
    "\n",
    "# for style.py\n",
    "# havent gottent there yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_samples(directory):\n",
    "    \"\"\"\n",
    "    From Keras DirectoryIterator\n",
    "    \"\"\"\n",
    "    return 290\n",
    "    classes = []\n",
    "    for subdir in sorted(os.listdir(directory)):\n",
    "        if os.path.isdir(os.path.join(directory, subdir)):\n",
    "            classes.append(subdir)\n",
    "\n",
    "    white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm'}\n",
    "    pool = multiprocessing.pool.ThreadPool()\n",
    "    function_partial = partial(_count_valid_files_in_directory,\n",
    "                               white_list_formats=white_list_formats,\n",
    "                               follow_links=False)\n",
    "    num_samples = sum(pool.map(function_partial,\n",
    "                               (os.path.join(directory, subdir)\n",
    "                                for subdir in classes)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return num_samples\n",
    "\n",
    "def convert_float32_to_uint8(img):\n",
    "    img = img.astype(np.float64) / np.amax(img) \n",
    "    img = 255 * img # Now scale by 255\n",
    "    img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpooling logic based on code from https://stackoverflow.com/questions/54960990/unpooling-in-keras-tf\n",
    "def mask_make(x, orig):\n",
    "    t = UpSampling2D()(x)\n",
    "    _,a,b,c = orig.shape \n",
    "    \n",
    "    xReshaped = Reshape((1,a*b*c))(t)\n",
    "    origReshaped = Reshape((1,a*b*c))(orig)\n",
    "\n",
    "    together = Concatenate(axis = -1)([origReshaped,xReshaped])\n",
    "    togReshaped = Reshape((2,a,b,c))(together)\n",
    "\n",
    "    bool_mask = Lambda(lambda t: K.greater_equal(t[:,0], t[:,1]))(togReshaped)\n",
    "\n",
    "    mask = Lambda(lambda t: K.cast(t, dtype='float32'))(bool_mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def vgg_layers(inputs, target_layer):\n",
    "    masks = []\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    orig = x \n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "    masks.append(mask_make(x, orig))\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    orig = x \n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "    masks.append(mask_make(x, orig))\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    orig = x \n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "    masks.append(mask_make(x, orig))\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    orig = x \n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "    masks.append(mask_make(x, orig))\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x, masks\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    f = h5py.File(WEIGHTS_PATH)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    layers, masks = vgg_layers(inputs, target_layer)\n",
    "    model = Model(inputs, layers, name='vgg19')\n",
    "    load_weights(model)\n",
    "    return model, masks\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 1:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 5:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.encoder, masks = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path, compile=False)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "\n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            encoder, masks = VGG19(inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 290 images belonging to 29 classes.\n",
      "290\n",
      "Epoch 1/2\n",
      " 1/72 [..............................] - ETA: 7:34 - loss: 36860190720.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3a13533168dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mOutputPreview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreview_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n\u001b[0;32m---> 58\u001b[0;31m         epochs=epochs, callbacks=callbacks)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "# This needs to be in scope where model is defined\n",
    "class OutputPreview(Callback):\n",
    "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        \n",
    "        # Resizing Image\n",
    "#         test_img = imresize(test_img, (256, 256, 3))\n",
    "#         test_img = resize(test_img, (256, 256, 3))\n",
    "        test_img = test_img.resize((256, 256)) # Assumes using 3 channels I think\n",
    "    \n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            \n",
    "            # Convert image to uint8 then save\n",
    "            output_img = convert_float32_to_uint8(output_img)\n",
    "            imageio.imwrite(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "num_samples = count_num_samples(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "print(num_samples)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "callbacks = [OutputPreview(encoder_decoder, image_path, 5000, preview_dir_path)]\n",
    "encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, callbacks=callbacks)\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate-decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(decoder_path=DECODER_PATH, target_layer=target_layer)\n",
    "\n",
    "input_img = image.load_img(INPUT_IMG_PATH)\n",
    "input_img = input_img.resize((256, 256))\n",
    "input_img = image.img_to_array(input_img)\n",
    "input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "output_img = encoder_decoder.model.predict([input_img])[0]\n",
    "\n",
    "# Convert image to uint8 then save\n",
    "output_img = convert_float32_to_uint8(output_img)\n",
    "imageio.imwrite(OUTPUT_IMG_PATH, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### style.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_features(vgg, inputs, target_layer):\n",
    "    output_layers = [\n",
    "            'block1_conv1',\n",
    "            'block2_conv1',\n",
    "            'block3_conv1',\n",
    "            'block4_conv1',\n",
    "            'block5_conv1'\n",
    "    ]\n",
    "\n",
    "    outputs = [layer.output for layer in vgg.layers\n",
    "               if layer.name == output_layers[target_layer-1]]\n",
    "    f = K.function([vgg.input] + [K.learning_phase()], outputs)\n",
    "    return f([inputs, 1.])\n",
    "\n",
    "\n",
    "def wct(content, style, alpha=0.6, eps=1e-5):\n",
    "    '''\n",
    "    https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "       Perform Whiten-Color Transform on feature maps using numpy\n",
    "       See p.4 of the Universal Style Transfer paper for equations:\n",
    "       https://arxiv.org/pdf/1705.08086.pdf\n",
    "    '''\n",
    "    # 1xHxWxC -> CxHxW\n",
    "    content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "    style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    # CxHxW -> CxH*W\n",
    "    content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "    style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "    # Whitening transform\n",
    "    mc = content_flat.mean(axis=1, keepdims=True)\n",
    "    fc = content_flat - mc\n",
    "    fcfc = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "    Ec, wc, _ = np.linalg.svd(fcfc)\n",
    "    k_c = (wc > 1e-5).sum()\n",
    "    Dc = np.diag((wc[:k_c]+eps)**-0.5)\n",
    "    fc_hat = Ec[:,:k_c].dot(Dc).dot(Ec[:,:k_c].T).dot(fc)\n",
    "\n",
    "    # Coloring transform\n",
    "    ms = style_flat.mean(axis=1, keepdims=True)\n",
    "    fs = style_flat - ms\n",
    "    fsfs = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "    Es, ws, _ = np.linalg.svd(fsfs)\n",
    "    k_s = (ws > 1e-5).sum()\n",
    "    Ds = np.sqrt(np.diag(ws[:k_s]+eps))\n",
    "    fcs_hat = Es[:,:k_s].dot(Ds).dot(Es[:,:k_s].T).dot(fc_hat)\n",
    "    fcs_hat = fcs_hat + ms\n",
    "\n",
    "    # Blend transform features with original features\n",
    "    blended = alpha*fcs_hat + (1 - alpha)*(fc)\n",
    "\n",
    "    # CxH*W -> CxHxW\n",
    "    blended = blended.reshape(content_t.shape)\n",
    "    # CxHxW -> 1xHxWxC\n",
    "    blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "    return np.float32(blended)\n",
    "\n",
    "\n",
    "\n",
    "img_c = image.load_img(sys.argv[1])\n",
    "img_c = image.img_to_array(img_c)\n",
    "img_c_shape = img_c.shape\n",
    "img_c = np.expand_dims(img_c, axis=0)\n",
    "\n",
    "img_s = image.load_img(sys.argv[2])\n",
    "img_s = image.img_to_array(img_s)\n",
    "img_s_shape = img_s.shape\n",
    "img_s = np.expand_dims(img_s, axis=0)\n",
    "\n",
    "assert img_c_shape == img_s_shape, \\\n",
    "    'Content and style image should be the same shape, %s != %s' \\\n",
    "    % (str(img_c_shape), str(img_s_shape))\n",
    "\n",
    "input_shape = img_c_shape\n",
    "\n",
    "print('Loading decoders...')\n",
    "decoders = {}\n",
    "decoders[1] = load_model('./models/decoder_1.h5')\n",
    "decoders[2] = load_model('./models/decoder_2.h5')\n",
    "decoders[3] = load_model('./models/decoder_3.h5')\n",
    "decoders[4] = load_model('./models/decoder_4.h5')\n",
    "decoders[5] = load_model('./models/decoder_5.h5')\n",
    "\n",
    "print('Loading VGG...')\n",
    "vgg, masks = VGG19(input_shape=input_shape, target_layer=5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "plt.show()\n",
    "\n",
    "print('Styling...')\n",
    "for i in [3, 1]:\n",
    "    feats_c = get_vgg_features(vgg, img_c, i)\n",
    "    feats_s = get_vgg_features(vgg, img_s, i)\n",
    "    feats_cs = wct(feats_c, feats_s)\n",
    "    img_c = decoders[i].predict(feats_cs)\n",
    "    plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "    plt.show()\n",
    "\n",
    "print('Saving output...')\n",
    "output_img = img_c[0]\n",
    "\n",
    "imsave(sys.argv[3], output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA's Photorealistic Smoothing\n",
    "Found here: https://github.com/NVIDIA/FastPhotoStyle/blob/master/photo_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (C) 2018 NVIDIA Corporation.    All rights reserved.\n",
    "Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import torch.nn as nn\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Propagator(nn.Module):\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super(Propagator, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def process(self, initImg, contentImg):\n",
    "\n",
    "        if type(contentImg) == str:\n",
    "            content = scipy.misc.imread(contentImg, mode='RGB')\n",
    "        else:\n",
    "            content = contentImg.copy()\n",
    "        # content = scipy.misc.imread(contentImg, mode='RGB')\n",
    "\n",
    "        if type(initImg) == str:\n",
    "            B = scipy.misc.imread(initImg, mode='RGB').astype(np.float64) / 255\n",
    "        else:\n",
    "            B = scipy.asarray(initImg).astype(np.float64) / 255\n",
    "            # B = self.\n",
    "        # B = scipy.misc.imread(initImg, mode='RGB').astype(np.float64)/255\n",
    "        h1,w1,k = B.shape\n",
    "        h = h1 - 4\n",
    "        w = w1 - 4\n",
    "        B = B[int((h1-h)/2):int((h1-h)/2+h),int((w1-w)/2):int((w1-w)/2+w),:]\n",
    "        content = scipy.misc.imresize(content,(h,w))\n",
    "        B = self.__replication_padding(B,2)\n",
    "        content = self.__replication_padding(content,2)\n",
    "        content = content.astype(np.float64)/255\n",
    "        B = np.reshape(B,(h1*w1,k))\n",
    "        W = self.__compute_laplacian(content)\n",
    "        W = W.tocsc()\n",
    "        dd = W.sum(0)\n",
    "        dd = np.sqrt(np.power(dd,-1))\n",
    "        dd = dd.A.squeeze()\n",
    "        D = scipy.sparse.csc_matrix((dd, (np.arange(0,w1*h1), np.arange(0,w1*h1)))) # 0.026\n",
    "        S = D.dot(W).dot(D)\n",
    "        A = scipy.sparse.identity(w1*h1) - self.beta*S\n",
    "        A = A.tocsc()\n",
    "        solver = scipy.sparse.linalg.factorized(A)\n",
    "        V = np.zeros((h1*w1,k))\n",
    "        V[:,0] = solver(B[:,0])\n",
    "        V[:,1] = solver(B[:,1])\n",
    "        V[:,2] = solver(B[:,2])\n",
    "        V = V*(1-self.beta)\n",
    "        V = V.reshape(h1,w1,k)\n",
    "        V = V[2:2+h,2:2+w,:]\n",
    "        \n",
    "        img = Image.fromarray(np.uint8(np.clip(V * 255., 0, 255.)))\n",
    "        return img\n",
    "\n",
    "    # Returns sparse matting laplacian\n",
    "    # The implementation of the function is heavily borrowed from\n",
    "    # https://github.com/MarcoForte/closed-form-matting/blob/master/closed_form_matting.py\n",
    "    # We thank Marco Forte for sharing his code.\n",
    "    def __compute_laplacian(self, img, eps=10**(-7), win_rad=1):\n",
    "            win_size = (win_rad*2+1)**2\n",
    "            h, w, d = img.shape\n",
    "            c_h, c_w = h - 2*win_rad, w - 2*win_rad\n",
    "            win_diam = win_rad*2+1\n",
    "            indsM = np.arange(h*w).reshape((h, w))\n",
    "            ravelImg = img.reshape(h*w, d)\n",
    "            win_inds = self.__rolling_block(indsM, block=(win_diam, win_diam))\n",
    "            win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
    "            winI = ravelImg[win_inds]\n",
    "            win_mu = np.mean(winI, axis=2, keepdims=True)\n",
    "            win_var = np.einsum('...ji,...jk ->...ik', winI, winI)/win_size - np.einsum('...ji,...jk ->...ik', win_mu, win_mu)\n",
    "            inv = np.linalg.inv(win_var + (eps/win_size)*np.eye(3))\n",
    "            X = np.einsum('...ij,...jk->...ik', winI - win_mu, inv)\n",
    "            vals = (1/win_size)*(1 + np.einsum('...ij,...kj->...ik', X, winI - win_mu))\n",
    "            nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
    "            nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
    "            nz_indsVal = vals.ravel()\n",
    "            L = scipy.sparse.coo_matrix((nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h*w, h*w))\n",
    "            return L\n",
    "\n",
    "    def __replication_padding(self, arr,pad):\n",
    "            h,w,c = arr.shape\n",
    "            ans = np.zeros((h+pad*2,w+pad*2,c))\n",
    "            for i in range(c):\n",
    "                    ans[:,:,i] = np.pad(arr[:,:,i],pad_width=(pad,pad),mode='edge')\n",
    "            return ans\n",
    "\n",
    "    def __rolling_block(self, A, block=(3, 3)):\n",
    "        shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
    "        strides = (A.strides[0], A.strides[1]) + A.strides\n",
    "        return as_strided(A, shape=shape, strides=strides)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
