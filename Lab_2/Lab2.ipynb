{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/WillLacey/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for utils.py\n",
    "import multiprocessing.pool\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# for vgg.py\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# for decoder.py\n",
    "from keras.layers import Input, Conv2D, UpSampling2D\n",
    "\n",
    "# for model.py \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Conv2D, Input\n",
    "import keras.backend as K\n",
    "\n",
    "# for train.py\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "# for evaluate-decoder.py\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# for style.py\n",
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vgg\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "WEIGHTS_PATH = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "\n",
    "# for model.py\n",
    "LAMBDA=1\n",
    "\n",
    "# for train.py \n",
    "TRAIN_PATH = 'data/input/101_ObjectCategories'\n",
    "TARGET_SIZE = (256, 256)\n",
    "BATCH_SIZE = 4\n",
    "epochs = 2\n",
    "target_layer = 2 # also for evaluate-decoder\n",
    "    # 1 - 8 hours\n",
    "    # 2 - 160 hours\n",
    "    # 3 - 80 hours\n",
    "    # 4 - 40 hours \n",
    "    # 5 - 21 hours \n",
    "\n",
    "image_path = \"data/input/content.png\"\n",
    "preview_dir_path = \"data/output\"\n",
    "\n",
    "# for evaluate-decoder.py\n",
    "DECODER_PATH = 'models/first_decoder_1.h5'\n",
    "INPUT_IMG_PATH = \"data/input/content.png\"\n",
    "OUTPUT_IMG_PATH = 'data/output/out.png'\n",
    "\n",
    "content_image_path = \"data/input/content.png\"\n",
    "style_image_path = INPUT_IMG_PATH\n",
    "output_image_path = \"data/output/style_transfer.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_images(path):\n",
    "    \"\"\"\n",
    "    Counts the number of files within a directory\n",
    "    \"\"\"\n",
    "    white_list_file_types = ['png', 'jpg', 'jpeg', 'bmp', 'ppm']\n",
    "    num_samples = 0\n",
    "    \n",
    "    path, directories, files = next(os.walk(path))\n",
    "    for directory in directories:\n",
    "        path_of_dir = path + '/' + directory\n",
    "        p, ds, fs = next(os.walk(path_of_dir))\n",
    "        for f in fs:\n",
    "            for file_type in white_list_file_types: \n",
    "                if file_type in f:\n",
    "                    num_samples += 1\n",
    "    return num_samples\n",
    "\n",
    "def convert_float32_to_uint8(img):\n",
    "    img = img.astype(np.float64) / np.amax(img) \n",
    "    img = 255 * img # Now scale by 255\n",
    "    img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(inputs, target_layer):\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n",
    "    if target_layer == 1:\n",
    "        return x\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    if target_layer == 2:\n",
    "        return x\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    if target_layer == 3:\n",
    "        return x\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    if target_layer == 4:\n",
    "        return x\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    f = h5py.File(WEIGHTS_PATH)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        b_name = layer.name.encode()\n",
    "        if b_name in layer_names:\n",
    "            g = f[b_name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def VGG19(input_tensor=None, input_shape=None, target_layer=1):\n",
    "    \"\"\"\n",
    "    VGG19, up to the target layer (1 for relu1_1, 2 for relu2_1, etc.)\n",
    "    \"\"\"\n",
    "    if input_tensor is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    else:\n",
    "        inputs = Input(tensor=input_tensor, shape=input_shape)\n",
    "    model = Model(inputs, vgg_layers(inputs, target_layer), name='vgg19')\n",
    "    load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    # Convert 'RGB' -> 'BGR'\n",
    "    if type(x) is np.ndarray:\n",
    "        x = x[..., ::-1]\n",
    "    else:\n",
    "        x = tf.reverse(x, [-1])\n",
    "\n",
    "    return x - MEAN_PIXEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, layer):\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block5_conv1')(inputs)\n",
    "    if layer == 1:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block4_upsample')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv4')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='decoder_block4_conv1')(x)\n",
    "    if layer == 2:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block3_upsample')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv4')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='decoder_block3_conv1')(x)\n",
    "    if layer == 3:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block2_upsample')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv2')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='decoder_block2_conv1')(x)\n",
    "    if layer == 4:\n",
    "        return x\n",
    "\n",
    "    x = UpSampling2D((2, 2), name='decoder_block1_upsample')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv2')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='decoder_block1_conv1')(x)\n",
    "    if layer == 5:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, input_shape=(256, 256, 3), target_layer=5,\n",
    "                 decoder_path=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.target_layer = target_layer\n",
    "\n",
    "        self.encoder = VGG19(input_shape=input_shape, target_layer=target_layer)\n",
    "        if decoder_path:\n",
    "            self.decoder = load_model(decoder_path, compile=False)\n",
    "        else:\n",
    "            self.decoder = self.create_decoder(target_layer)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.encoder)\n",
    "        self.model.add(self.decoder)\n",
    "\n",
    "        self.loss = self.create_loss_fn(self.encoder)\n",
    "\n",
    "        self.model.compile('adam', self.loss)\n",
    "\n",
    "    def create_loss_fn(self, encoder):\n",
    "        def get_encodings(inputs):\n",
    "            encoder = VGG19(inputs, self.input_shape, self.target_layer)\n",
    "            return encoder.output\n",
    "\n",
    "        def loss(img_in, img_out):\n",
    "            encoding_in = get_encodings(img_in)\n",
    "            encoding_out = get_encodings(img_out)\n",
    "            return l2_loss(img_out - img_in) + \\\n",
    "                   LAMBDA*l2_loss(encoding_out - encoding_in)\n",
    "        return loss\n",
    "\n",
    "    def create_decoder(self, target_layer):\n",
    "        inputs = Input(shape=self.encoder.output_shape[1:])\n",
    "        layers = decoder_layers(inputs, target_layer)\n",
    "        output = Conv2D(3, (3, 3), activation='relu', padding='same',\n",
    "                        name='decoder_out')(layers)\n",
    "        return Model(inputs, output, name='decoder_%s' % target_layer)\n",
    "\n",
    "    def export_decoder(self):\n",
    "        self.decoder.save('decoder_%s.h5' % self.target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9144 images belonging to 102 classes.\n",
      "9144\n",
      "Epoch 1/2\n",
      "   1/2286 [..............................] - ETA: 67:11:09 - loss: 273122213888.0000"
     ]
    }
   ],
   "source": [
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            if img.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            # (X, y)\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "# This needs to be in scope where model is defined\n",
    "class OutputPreview(Callback):\n",
    "    def __init__(self, model, test_img_path, increment, preview_dir_path):\n",
    "        test_img = image.load_img(test_img_path)\n",
    "        \n",
    "        # Resizing Image\n",
    "#         test_img = imresize(test_img, (256, 256, 3))\n",
    "#         test_img = resize(test_img, (256, 256, 3))\n",
    "        test_img = test_img.resize((256, 256)) # Assumes using 3 channels I think\n",
    "    \n",
    "        test_target = image.img_to_array(test_img)\n",
    "        test_target = np.expand_dims(test_target, axis=0)\n",
    "        self.test_img = test_target\n",
    "        self.model = model\n",
    "\n",
    "        self.preview_dir_path = preview_dir_path\n",
    "\n",
    "        self.increment = increment\n",
    "        self.iteration = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if (self.iteration % self.increment == 0):\n",
    "            output_img = self.model.predict(self.test_img)[0]\n",
    "            fname = '%d.jpg' % self.iteration\n",
    "            out_path = os.path.join(self.preview_dir_path, fname)\n",
    "            \n",
    "            # Convert image to uint8 then save\n",
    "            output_img = convert_float32_to_uint8(output_img)\n",
    "            imageio.imwrite(out_path, output_img)\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "gen = create_gen(TRAIN_PATH, TARGET_SIZE, BATCH_SIZE)\n",
    "\n",
    "num_samples = count_number_of_images(TRAIN_PATH)\n",
    "steps_per_epoch = num_samples // BATCH_SIZE\n",
    "print(num_samples)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(target_layer=target_layer)\n",
    "\n",
    "callbacks = [OutputPreview(encoder_decoder, image_path, 5000, preview_dir_path)]\n",
    "encoder_decoder.model.fit_generator(gen, steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, callbacks=callbacks)\n",
    "encoder_decoder.export_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate-decoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(decoder_path=DECODER_PATH, target_layer=target_layer)\n",
    "\n",
    "input_img = image.load_img(INPUT_IMG_PATH)\n",
    "input_img = input_img.resize((256, 256))\n",
    "input_img = image.img_to_array(input_img)\n",
    "input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "output_img = encoder_decoder.model.predict([input_img])[0]\n",
    "\n",
    "# Convert image to uint8 then save\n",
    "output_img = convert_float32_to_uint8(output_img)\n",
    "imageio.imwrite(OUTPUT_IMG_PATH, output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### style.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_features(vgg, inputs, target_layer):\n",
    "    output_layers = [\n",
    "            'block1_conv1',\n",
    "            'block2_conv1',\n",
    "            'block3_conv1',\n",
    "            'block4_conv1',\n",
    "            'block5_conv1'\n",
    "    ]\n",
    "\n",
    "    outputs = [layer.output for layer in vgg.layers\n",
    "               if layer.name == output_layers[target_layer-1]]\n",
    "    f = K.function([vgg.input] + [K.learning_phase()], outputs)\n",
    "    return f([inputs, 1.])\n",
    "\n",
    "\n",
    "def wct(content, style, alpha=0.6, eps=1e-5):\n",
    "    '''\n",
    "    https://github.com/eridgd/WCT-TF/blob/master/ops.py\n",
    "       Perform Whiten-Color Transform on feature maps using numpy\n",
    "       See p.4 of the Universal Style Transfer paper for equations:\n",
    "       https://arxiv.org/pdf/1705.08086.pdf\n",
    "    '''\n",
    "    # 1xHxWxC -> CxHxW\n",
    "    content_t = np.transpose(np.squeeze(content), (2, 0, 1))\n",
    "    style_t = np.transpose(np.squeeze(style), (2, 0, 1))\n",
    "\n",
    "    # CxHxW -> CxH*W\n",
    "    content_flat = content_t.reshape(-1, content_t.shape[1]*content_t.shape[2])\n",
    "    style_flat = style_t.reshape(-1, style_t.shape[1]*style_t.shape[2])\n",
    "\n",
    "    # Whitening transform\n",
    "    mc = content_flat.mean(axis=1, keepdims=True)\n",
    "    fc = content_flat - mc\n",
    "    fcfc = np.dot(fc, fc.T) / (content_t.shape[1]*content_t.shape[2] - 1)\n",
    "    Ec, wc, _ = np.linalg.svd(fcfc)\n",
    "    k_c = (wc > 1e-5).sum()\n",
    "    Dc = np.diag((wc[:k_c]+eps)**-0.5)\n",
    "    fc_hat = Ec[:,:k_c].dot(Dc).dot(Ec[:,:k_c].T).dot(fc)\n",
    "\n",
    "    # Coloring transform\n",
    "    ms = style_flat.mean(axis=1, keepdims=True)\n",
    "    fs = style_flat - ms\n",
    "    fsfs = np.dot(fs, fs.T) / (style_t.shape[1]*style_t.shape[2] - 1)\n",
    "    Es, ws, _ = np.linalg.svd(fsfs)\n",
    "    k_s = (ws > 1e-5).sum()\n",
    "    Ds = np.sqrt(np.diag(ws[:k_s]+eps))\n",
    "    fcs_hat = Es[:,:k_s].dot(Ds).dot(Es[:,:k_s].T).dot(fc_hat)\n",
    "    fcs_hat = fcs_hat + ms\n",
    "\n",
    "    # Blend transform features with original features\n",
    "    blended = alpha*fcs_hat + (1 - alpha)*(fc)\n",
    "\n",
    "    # CxH*W -> CxHxW\n",
    "    blended = blended.reshape(content_t.shape)\n",
    "    # CxHxW -> 1xHxWxC\n",
    "    blended = np.expand_dims(np.transpose(blended, (1,2,0)), 0)\n",
    "\n",
    "    return np.float32(blended)\n",
    "\n",
    "\n",
    "\n",
    "img_c = image.load_img(content_image_path)\n",
    "img_c = img_c.resize((256, 256)) # Assumes using 3 channels I think\n",
    "img_c = image.img_to_array(img_c)\n",
    "img_c_shape = img_c.shape\n",
    "img_c = np.expand_dims(img_c, axis=0)\n",
    "\n",
    "img_s = image.load_img(style_image_path)\n",
    "img_s = img_s.resize((256, 256)) # Assumes using 3 channels I think\n",
    "img_s = image.img_to_array(img_s)\n",
    "img_s_shape = img_s.shape\n",
    "img_s = np.expand_dims(img_s, axis=0)\n",
    "\n",
    "assert img_c_shape == img_s_shape, \\\n",
    "    'Content and style image should be the same shape, %s != %s' \\\n",
    "    % (str(img_c_shape), str(img_s_shape))\n",
    "\n",
    "input_shape = img_c_shape\n",
    "\n",
    "print('Loading decoders...')\n",
    "decoders = {}\n",
    "decoders[1] = load_model('./models/decoder_1.h5', compile=False)\n",
    "# decoders[2] = load_model('./models/decoder_2.h5', compile=False)\n",
    "# decoders[3] = load_model('./models/decoder_3.h5', compile=False)\n",
    "# decoders[4] = load_model('./models/decoder_4.h5', compile=False)\n",
    "# decoders[5] = load_model('./models/decoder_5.h5', compile=False)\n",
    "\n",
    "print('Loading VGG...')\n",
    "vgg = VGG19(input_shape=input_shape, target_layer=5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "plt.show()\n",
    "\n",
    "print('Styling...')\n",
    "for i in [3, 1]:\n",
    "    feats_c = get_vgg_features(vgg, img_c, i)\n",
    "    feats_s = get_vgg_features(vgg, img_s, i)\n",
    "    feats_cs = wct(feats_c, feats_s)\n",
    "    img_c = decoders[i].predict(feats_cs)\n",
    "    plt.imshow(np.clip(img_c[0] / 255, 0, 1))\n",
    "    plt.show()\n",
    "\n",
    "print('Saving output...')\n",
    "output_img = img_c[0]\n",
    "\n",
    "output_img = convert_float32_to_uint8(output_img)\n",
    "imageio.imwrite(output_image_path, output_img)\n",
    "print('Saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
